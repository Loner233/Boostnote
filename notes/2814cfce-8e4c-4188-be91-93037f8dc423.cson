createdAt: "2019-04-18T10:45:12.515Z"
updatedAt: "2019-06-13T02:32:41.925Z"
type: "MARKDOWN_NOTE"
folder: "91678286fbe5c256d96d"
title: "GradientTape"
tags: []
content: '''
  ### GradientTape
  OP如果在上下文管理器中被执行的话，它就会被记录下来，至少输入inputs中的一个会被监听（watched）
  
  可训练变量（由`tf.Variable`或者`tf.compat.v1.get_variable`创建）会被自动监听（watched),张量也可以被手动监听——在上下文管理器中使用`watch`方法。例如，函数$y = x * x$,想要获取$x = 3.0$时，y的导数值则可以：
  
  
  ```python
  x = tf.constant(3.0)
  with tf.GradientTape() as g:
      g.watch(x)
      y = x * x
  dy_dx = g.gradient(y,x)
  ```
  
  GradientTapes 也可以被嵌套来计算高阶导数
  ```python
  x = tf.constant(3.0)
  with tf.GradientTape()  as g:
      g.watch(x)
      with tf.GradientTape() as gg:
          gg.watch(x)
          y = x * x
      dy_dx = gg.gradient(y,x)
  d2y_dx2 = g.gradient(dy_dx,x)
  ```
  
  默认情况下，在`GradientTape.gradient()`调用后，由`GradientTape`控制的资源将立即被释放，所以如果想计算多个梯度的化，需要创建一个`persistent`的`GradientTape`。这个东西允许多次调用`gradient()`，**当GC时，GradientTape的资源才会被释放**
  ```python
  x = tf.constant(3.0)
  with tf.GradientTape(persistent=True) as g:
      g.watch(x)
      y = x * x
      z = y * y
  dz_dx = g.gradient(z,x)
  dy_dx = g.gradient(y,x)
  del g     #千万要记得手动GC哈！！！！
  ```
  
  默认情况下GradientTape会watch所有的可训练变量(实测没有watch啊，有待进一步考证)，如果想要关闭此功能，可以在构造GradientTape的时候添加`watch_accessed_variables=False`
  ```python
  with tf.GradientTape(watch_accessed_variables=False) as tape:
      tape.watch(variable_a)
      y = variable_a ** 2 #梯度可获得
      z = variable_b ** 2 #梯度不可获得，因为它没有被watch
  ```
  
  注意点：
  在使用`watch_accessed_variables=False`时，一定要确保模型变量的存在
  ```python
  a = tf.keras.layers.Dense(32)
  b = tf.keras.layers.Dense(32)
  with tf.GradientTape(watch_accessed_variables=False) as tape:
      tape.watch(a.variables)
      result = b(a(inputs))
  tape.gradient(result,a.variables)    
  ```
  以上导数求出类一个全是None的List，因为a的变量没有被watch。
  为什么a的变量没有被watch呢？
  因为没有调用a模型的build，所以a.variables会返回一个空的list，并且使用了`watch_accessed_variables=False`,所以tape不会watch任何东西。
'''
linesHighlighted: []
isStarred: false
isTrashed: false
