createdAt: "2019-09-25T01:44:21.857Z"
updatedAt: "2019-09-25T02:40:48.902Z"
type: "MARKDOWN_NOTE"
folder: "cab491cd31795b9833d0"
title: "Skip Connections"
tags: []
content: '''
  # Skip Connections
  深度神经网络非常难以训练，尤其是会面对一些梯度消失和梯度爆炸的问题，此时**Batch Normalization**能够一定程度缓解，但是也带来了另一个问题，随着深度的增加，神经网络收敛难度也增加了
  
  ## Residual Block
  这个概念是在**ResNet**（Residual Network）中提到的，残差网络在不增加网络深度的基础上，通过用*Residual Block*来构建网络，从而实现了一定程度上的避免过拟合和梯度消失。
  
  Residual Block是一个三层的卷积神经网络（以下仅以普通神经网络表示，其实也没差别），其中$a$表示第$i$层的输出，$F_{i+1}$表示第$i+1$层的激活函数，$W$和$B$分别表示权重和偏置。
  ![Untitled Diagram (1).png](:storage/b5ea5ab1-ab63-452b-9f21-df5cce16ce80/9aa43e0b.png)
  
  显然在第$i+2$层的输入并不只有第$i+1$层的输入，还加上第$i$层的输出，这样的一块就叫做残差块，而残差神经网络则是有好多的残差块组成。
  ###### tip:
  由于第$i+2$层的输入要加上第$i$层的输出，这就要求第$i+2$层的神经元个数要和第$i$层的神经元的个数是相同，而针对于CNN就要保证第$i+2$层的shape要和第$i$层的shape是一致的，如果不一致，则可以通过添加一层全连接层来使其保持一致。
  
  同时这种，跳过一层，把输出直接传递给下下层的技术叫做**Skip Connections**,它能够缓解网络层数变多的情况下导致的梯度爆炸和梯度消失。
'''
linesHighlighted: []
isStarred: false
isTrashed: false
