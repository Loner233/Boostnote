createdAt: "2019-05-30T02:32:35.265Z"
updatedAt: "2019-06-13T02:34:21.997Z"
type: "MARKDOWN_NOTE"
folder: "91678286fbe5c256d96d"
title: "低阶api"
tags: []
content: '''
  ## 低阶api
  最近用了几次tensorflow的高阶api，发现构建自己的算法的时候还是有很大的局限性，再用回低阶api的时候发现自己蠢哭了，竟然不会写了，乘此机会复习复习，顺便看看新的特性。
  
  ### 张量值
  ```python
  3. # a rank 0 tensor; a scalar with shape [],
  [1., 2., 3.] # a rank 1 tensor; a vector with shape [3]
  [[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
  [[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
  ```
  
  ### TensorFlow core
  TensorFlow Core可以看作由两个互相地理的部分组成：
  - 构建计算图(tf.Graph)
  - 运行计算图(tf.Session)
  
  #### 图
  计算图是排列成一个Graph的一系列TF指令，图主要由两种类型的对象组成
  - 操作（op）：图的节点，描述了消耗和生成张量的计算
  - 张量：图的边，代表将流经节点的值。大多数TF函数返回`tf.Tensors`
  tips:`tf.Tensors`不具有值，他只是计算图中元素的手柄
  ```python
  import tensorflow as tf
  a = tf.constant(3.0,dtype=tf.float32)
  b = tf.constant(4.0) #type隐式赋值为float32
   total = a + b
  print(a)
  #Tensor("Const:0", shape=(), dtype=float32)
  print(b)
  #Tensor("Const_1:0", shape=(), dtype=float32)
  print(total)
  #Tensor("add:0", shape=(), dtype=float32)
  ```
  以上代码并没有输出详细的值，因为上述语句只是构建了计算图，图中每一个指定都拥有唯一的名称。这个名称不同于使用Python分配给相应对象的名称。
  张量是根据生成他们的指令命名的，后面整个输出索引。
  
  ### TensorBoard
  TensorFlow可以使用TensorBoard来将计算图可视化
  - 首先将计算图保存为TensorBoard摘要文件
  ```python
  writer = tf.summary.FileWriter('.')
  writer.add_graph(tf.get_default_graph())
  ```
  上述将会在当前目录下面生成一个event文件，其名称格式为
  **event.out.tfevents.{timestamp}.{hostname}**
  
  现在，在新的终端中启动TensorBoard
  ```bash
  tensorboard --logdir . --host localhost
  ```
  就能获得如下的计算图
  ![graph_run= (3).png](:storage\\9fcac9ab-08e6-4cc7-8e5e-b5f20cb1176f\\554ada5e.png)
  
  #### 会话(Session)
  若要评估张量，需要实例化一个`tf.Session`对象。session会封装TensorFlow运行时的状态，并运行TensorFlow操作。
  以下代码会创建一个tf.Session对象，然后调用run方法来评估上文创建的total张量：
  ```python
  sess = tf.Session()
  print(sess.run(total))
  ```
  可以将多个张量传递给tf.Seession.run。run方法以透明方式处理元组或字典的任何组合
  ```python
  print(sess.run({'ab':(a,b),'total':total}))
  ```
  返回结构具有相同的布局结构
  {'ab': (3.0, 4.0), 'total': 7.0}
  
  在调用`tf.Session.run`期间，任何`tf.Tensor`都只有单个值
  ```python
  vec = tf.random_uniform(shape=(3,))
  
  out1 = vec + 1
  
  out2 = vec + 2
  
  print(sess.run(vec))
  # [0.54000616 0.82741916 0.042135  ]
  
  print(sess.run(vec))
  # [0.17032683 0.22657871 0.35737348]
  
  print(sess.run((out1,out2)))
  # (array([1.0402412, 1.1601874, 1.9776638], dtype=float32), array([2.0402412, 2.1601872, 2.9776638], dtype=float32))
  
  ```
  tip:部分函数返回tf.Operations，而不是tf.Tensors。对指令调用run的结果时None。运行指令是为了产生一个副作用，而不是为了获取一个值。
  
  
  #### 供给
  图可以参数化一边接受外部输入，也称为占位符。占位符表示承诺在稍后提供值，他就像函数参数。
  ```python
  x = tf.placeholder(tf.float32)
  y = tf.placeholder(tf.float32)
  z = x + y
  ```
  可以通过run方法的feed_dict参数来为占位符提供具体的值，从而评估这个具有多个输入的图：
  ```python
  print(sess.run(z,feed_dict={x:3,y:4.5}))
  # 7.5
  print(sess.run(z,feed_dict={x:[1,3],y:[2,4]}))
  # [ 3.  7.]
  ```
  tip:feed_dict参数可一个用于覆盖图中的任何张量。占位符和其他tf.Tensors的唯一不同之处在于让如果没有为占位符提供值，那么占位符会抛出错误。
  
  #### 数据集
  占位符适用于简单的实验，而数据集是将数据流式传输到模型的首选方式。
  ```python
  data = [[0, 1], [2, 3], [4, 5], [6, 7]]
  slices = tf.data.Dataset.from_tensor_slices(data)
  next_item = slices.make_one_shot_iterator().get_next()
  while True:
      try:
          print(sess.run(next_item))
      except tf.errors.OutOfRangeError:
          break
  ```
  如果Dataset依赖于有状态操作，则可能需要在使用迭代器之前先初始化它
  ```python
  r = tf.random_normal([10,3])
  dataset = tf.data.Dataset.from_tensor_slices(r)
  iterator = dataset.make_initializable_iterator()
  next_row = iterator.get_next()
  
  sess.run(iterator.initializer)
  while True:
    try:
      print(sess.run(next_row))
    except tf.errors.OutOfRangeError:
      break
  ```
  
  #### 层
  ##### 创建层
  ```python
  x = tf.placeholder(tf.float32, shape=[None, 3])
  linear_model = tf.layers.Dense(units=1)
  y = linear_model(x)
  ```
  初始化层
  ```python
  init = tf.global_variables_initializer()
  sess.run(init)
  ```
  tips:
  - `global_variables_initializer`仅会创建并返回TF操作的句柄，当我们使用tf.Session.run运行该操作是，该操作将初始化所有全局变量。
  - `golbal_variables_initializer`仅会初始化创建初始化程序时图中就存在的变量，因此在图标的最后一步添加初始化程序
  
  ##### 执行层
  ```python
  print(sess.run(y,{x:[[1,2,3],[4,5,6]]}))
  ```
  会生成一个两元素输出向量
  [[1.3859153]
   [2.026245 ]]
   ##### 层函数的快捷方式
  对于每个层类（如tf.layers.Dense），TensorFlow还提供了一个快捷函数（如tf.layers.dense）。两者唯一的区别就是快捷函数版本是在单次调用中创建和运行层。
  ```python
  x = tf.placeholder(tf.floatt32,shape=[None,3])
  y = tf.layerrs.dense(x,units=1)
  init = tf.global_variables_initializer()
  sess.run(initt)
  print(sess.run(y.{x:[[1,2,3],[4,5,6]]}))
  ```
  tip:虽然简单，但是没法访问tf.layers.Layers对象，这使得调试等变得更加困难，且无法重复使用相应的层。
  
  ##### 训练
  - 定义数据
  ```python
  x = tf.constant([[1],[2],[3],[4]],dtype=tf.float32)
  y_true = tf.constant([[0],[-1],[-2],[-3]],dtype=tf.float32)
  ```
  - 定义模型
   ```python
   linear_model = tf.layers.Dense(units=1)
   y_pred = linear_model(x)
   ```
   可以如下评估预测值
   ```python
   sess = tf.Session()
   init = tf.global_variables_initializer()
   sess.run(init)
   print(sess.run(y_pred))
   ```
  - 损失
   ```python
   loss = tf.losses.mean_squared_error(labels=y_true,predictions=y_pred)
   print(sess.run(loss))
   ```
  - 训练
  ```python
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize(loss)
  ```
  以上构建了优化方案
  ```python
  for i in range(10):
      _,loss_value = sess.run((train,loss))
      print(loss_value)
  ```
  
'''
linesHighlighted: []
isStarred: false
isTrashed: false
